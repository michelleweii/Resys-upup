线上服务是最关键的部分

09 （SparrowRecSys代码）

一个工业级的推荐服务器内部究竟都做了哪些事情？

线上服务模块的功能非常繁杂，它不仅需要跟离线训练好的模型打交道，把离线模型进行上线，在线进行模型服务（Model Serving），还需要跟数据库打交道，把候选物品和离线处理好的特征载入到服务器。而且线上服务器内部的逻辑也十分地复杂，不仅包括了一些经典的过程，比如召回层和排序层，还包括一些业务逻辑，比如照顾推荐结果多样性，流行度的一些硬性的混合规则，甚至还包括了一些 AB 测试相关的测试代码。



高并发推荐服务的整体架构

高并发推荐服务的整体架构主要由三个重要机制支撑，它们分别是负载均衡、缓存、推荐服务降级机制。

负载均衡：

“负载均衡”解决高并发的思路是“增加劳动力”，那我们能否从“减少劳动量”的角度来解决高并发带来的负载压力呢？

缓存：

比如说，当同一个用户多次请求同样的推荐服务时，我们就可以在第一次请求时把 TA 的推荐结果缓存起来，在后续请求时直接返回缓存中的结果就可以了，不用再通过复杂的推荐逻辑重新算一遍。再比如说，对于新用户来说，因为他们几乎没有行为历史的记录，所以我们可以先按照一些规则预先缓存好几类新用户的推荐列表，等遇到新用户的时候就直接返回。

但不管再强大的服务集群，再有效的缓存方案，也都有可能遭遇特殊时刻的流量洪峰或者软硬件故障。在这种特殊情况下，为了防止推荐服务彻底熔断崩溃，甚至造成相关微服务依次崩溃的“雪崩效应”，我们就要在第一时间将问题控制在推荐服务内部，而应对的最好机制就是“服务降级”。

所谓“服务降级”就是抛弃原本的复杂逻辑，采用最保险、最简单、最不消耗资源的降级服务来渡过特殊时期。比如对于推荐服务来说，我们可以抛弃原本的复杂推荐模型，采用基于规则的推荐方法来生成推荐列表，甚至直接在缓存或者内存中提前准备好应对故障时的默认推荐列表，做到“0”计算产出服务结果，这些都是服务降级的可行策略。



**“负载均衡”提升服务能力，“缓存”降低服务压力，“服务降级”机制保证故障时刻的服务不崩溃，压力不传导**，这三点可以看成是一个成熟稳定的高并发推荐服务的基石。

总结：

**每个注册到 Jetty Context 的 Servlet 服务中的主要业务逻辑？**

![img](https://static001.geekbang.org/resource/image/9f/df/9f756f358d1806dc9b3463538567d7df.jpeg)

疑问？

线上服务如何做到模型实时更新的呢？模型实时更新要用到模型serving的服务。

离线推荐和在线推荐的请求方式是什么？





### 10 存储模块 （代码）

> 类似 Embedding 这样的特征是在离线环境下生成的，而推荐服务器是在线上环境中运行的，那<u>这些离线的特征数据是如何导入到线上让推荐服务器使用的呢？</u>

Netflix 采用了非常经典的 Offline、Nearline、Online 三层推荐系统架构。架构图中最核心的位置就是我在图中用红框标出的部分，它们是三个数据库 Cassandra、MySQL 和 EVcache，**这三个数据库就是 Netflix 解决特征和模型参数存储问题的钥匙。**

![img](https://static001.geekbang.org/resource/image/bc/ca/bc6d770cb20dfc90cc07168d626fd7ca.jpg)

​                                                      图1 Netflix推荐系统架构中的特征与模型数据库



> 简而言之，存储推荐特征和模型就是找一个数据库把离线的特征存起来，然后再给推荐服务器写几个 SQL 让它取出来用。

对于推荐服务器来说，由于线上的 QPS 压力巨大，每次有推荐请求到来，推荐服务器都需要把相关的特征取出。这就要求推荐服务器一定要“快”。对于一个成熟的互联网应用来说，它的用户数和物品数一定是巨大的，几千万上亿的规模是十分常见的。所以对于存储模块来说，这么多用户和物品特征所需的存储量会特别大。这个时候，事情就很难办了，**又要存储量大，又要查询快，还要面对高 QPS 的压力**。很不幸，**没有一个独立的数据库能经济又高效地单独完成这样复杂的任务**。

> **几乎所有的工业级推荐系统都会做一件事情，就是把特征的存储做成分级存储，把越频繁访问的数据放到越快的数据库甚至缓存中，把海量的全量数据放到便宜但是查询速度较慢的数据库中。**

举个不恰当的例子，如果你把特征数据放到基于 HDFS 的 HBase 中，虽然你可以轻松放下所有的特征数据，但要让你的推荐服务器直接访问 HBase 进行特征查询，等到查询完成，这边用户的请求早就超时中断了，而 Netflix 的三个数据库正好满足了这样分级存储的需求。

![img](https://static001.geekbang.org/resource/image/03/78/0310b59276fde9eeec5d9cd946fef078.jpeg)

图2 分级存储的设计

比如说，Netflix 使用的 **Cassandra，它作为流行的 NoSQL 数据库，具备大数据存储的能力，但为支持推荐服务器高 QPS 的需求**，我们还需要把最常用的特征和模型参数存入 EVcache 这类内存数据库。而对于更常用的数据，我们可以把它们存储在 Guava Cache 等服务器内部缓存，甚至是服务器的内存中。总之，对于一个工程师来说，我们经常需要做出技术上的权衡，达成一个在花销和效果上平衡最优的技术方案。

而对于 **MySQL 来说，由于它是一个强一致性的关系型数据库，一般存储的是比较关键的要求强一致性的信息，比如物品是否可以被推荐这种控制类的信息，物品分类的层级关系，用户的注册信息等等。**这类信息一般是由推荐服务器进行阶段性的拉取，或者利用分级缓存进行阶段性的更新，避免因为过于频繁的访问压垮 MySQL。

推荐系统存储模块的设计原则就是<u>**“分级存储，把越频繁访问的数据放到越快的数据库甚至缓存中，把海量的全量数据放到廉价但是查询速度较慢的数据库中”**。</u>



SparrowRecsys 的存储系统方案

我们使用基础的文件系统保存全量的离线特征和模型数据，用 Redis 保存线上所需特征和模型数据，使用服务器内存缓存频繁访问的特征。

![img](https://static001.geekbang.org/resource/image/34/63/34958066e8704ea2780d7f8007e18463.jpeg)

​																								特征和模型数据

根据上面的特征数据，我们一起做一个初步的分析。首先，用户特征的总数比较大，它们很难全部载入到服务器内存中，所以我们把用户特征载入到 Redis 之类的内存数据库中是合理的。其次，物品特征的总数比较小，而且每次用户请求，一般只会用到一个用户的特征，但为了物品排序，推荐服务器需要访问几乎所有候选物品的特征。针对这个特点，我们完全可以把所有物品特征阶段性地载入到服务器内存中，大大减少 Redis 的线上压力。

最后，我们还要找一个地方去存储特征历史数据、样本数据等体量比较大，但不要求实时获取的数据。这个时候分布式文件系统（单机环境下以本机文件系统为例）往往是最好的选择，由于类似 HDFS 之类的分布式文件系统具有近乎无限的存储空间，我们可以把每次处理的全量特征，每次训练的 Embedding 全部保存到分布式文件系统中，方便离线评估时使用。

![img](https://static001.geekbang.org/resource/image/34/63/34958066e8704ea2780d7f8007e18463.jpeg)

文件系统的存储操作非常简单，在 SparrowRecsys 中就是利用 Spark 的输出功能实现的，我们就不再重点介绍了。而服务器内部的存储操作主要是跟 Redis 进行交互。



### Redis 基础知识

服务器内部的存储操作主要是跟 Redis 进行交互，Redis 是当今业界最主流的内存数据库。

**一是所有的数据都以 Key-value 的形式存储。** 其中，Key 只能是字符串，value 可支持的数据结构包括 string(字符串)、list(链表)、set(集合)、zset(有序集合) 和 hash(哈希)。这个特点决定了 Redis 的使用方式，无论是存储还是获取，都应该以键值对的形式进行，并且根据你的数据特点，设计值的数据结构。

**二是所有的数据都存储在内存中，磁盘只在持久化备份或恢复数据时起作用。**这个特点决定了 Redis 的特性，一是 QPS 峰值可以很高，二是数据易丢失，所以我们在维护 Redis 时要充分考虑数据的备份问题，或者说，不应该把关键的业务数据唯一地放到 Redis 中。但对于可恢复，不关乎关键业务逻辑的推荐特征数据，就非常适合利用 Redis 提供高效的存储和查询服务。



`如果你已经安装好了 Redis，我非常推荐你运行 SparrowRecsys 中 Offline 部分 Embedding 主函数，先把物品和用户 Embedding 生成并且插入 Redis（注意把 saveToRedis 变量改为 true）。然后再运行 Online 部分的 RecSysServer，看一下推荐服务器有没有正确地从 Redis 中读出物品和用户 Embedding 并产生正确的推荐结果（注意，记得要把 util.Config 中的 EMB_DATA_SOURCE 配置改为 DATA_SOURCE_REDIS）。`



![img](https://static001.geekbang.org/resource/image/5f/08/5f76090e7742593928eaf118d72d2b08.jpeg)



问题：存储 Embedding 的方式还有优化的空间吗？除了 string，我们是不是还可以用其他 Redis value 的数据结构存储 Embedding 数据，那从效率的角度考虑，使用 string 和使用其他数据结构的优缺点有哪些？为什么？

1.redis这种缓存中尽量放活跃的数据，存放全量的embedding数据，对内存消耗太大。尤其物品库，用户embedding特别多的情况下。
2.分布式kv可以做这种embedding的存储
3.关于embedding的编码可以用pb来解决。embedding维度太大的时候，redis里的数据结构占用空间会变大，因为除了embedding本身的空间，还有数据结构本身占用的空间。



- redis keys命令不能用在生产环境中，如果数量过大效率十分低，导致redis长时间堵塞在keys上。

作者回复: 非常好的点。生产环境我们一般选择提前载入一些warm up物品id的方式载入物品embedding。这里做了一个简化，推荐大家参考这条评论，多谢！

- Redis value 可以用pb格式存储, 存储上节省空间. 解析起来相比string, cpu的效率也应该会更高

作者回复: 生产环境确实经常使用protobuf进行压缩，非常好的经验。

protobuf 格式缺失是可以大幅压缩存储，节约存储空间，但是相对应的数据解压和可读性上(调试时)是有折扣的。

protobuf需要反序列化压缩数据，效率上不一定比json高，主要的benefit在于大幅节省存储空间。

- 老师，有俩个问题
  1，文中关于RecForYou，是来一个用户访问，就把用户的embding存入推荐服务器内存，如果一个短时间一下来百万级用户，都存入服务器内存，这样会不会出问题，优化的话应当也可以对用户分级，活跃用户存下来，非活跃其他还是从Redis实时读取用户特征。
  2，RecForYou中，**给用户推荐电影，使用的用户embding和候选电影embding的余弦距离来排序，这俩个不同维度embding计算余弦相似度有意义嘛，**还是因为本例子中用户embding由其看过的电影embbding 相加来的。所以这么做嘛

  作者回复: 这两个问题都是非常好的问题，推荐其他同学思考。

  \1. 我们并没有把用户embedding保存在内存中，只是把item embedding提前load到内存里，所以其实不存在这样的情况。但你说的也是非常好的用户数据缓存的方案，我们一般会指定一个用户内存区域的大小，用FIFO的方案来缓存，这样内存用完了，就自动把早进来的用户pop出去。

  另外分级的想法也非常好，如果有条件可以判断活跃用户，可以尽量选择活跃用户进行缓存。

  2、你说的没错，**用户emb和物品emb必须在一个向量空间内才能够做相似度计算。**咱们项目中的用户emb是通过item emb平均生成的，所以可以这样计算。

  

# 线上召回

在推荐物品候选集规模非常大的时候，我们该如何快速又准确地筛选掉不相关物品，从而节约排序时所消耗的计算资源呢？这其实就是推荐系统召回层要解决的问题。

从技术架构的角度来说，“召回层”处于推荐系统的线上服务模块之中，推荐服务器从数据库或内存中拿到所有候选物品集合后，会依次经过召回层、排序层、再排序层（也被称为补充算法层），才能够产生用户最终看到的推荐列表。既然线上服务需要这么多“层”才能产生最终的结果，不同层之间的功能特点有什么区别呢？

![img](https://static001.geekbang.org/resource/image/b1/6b/b1fd054eb2bbe0ec1237fc316byye66b.jpeg)

召回层就是要快速、准确地过滤出相关物品，缩小候选集，排序层则要以提升推荐效果为目标，作出精准的推荐列表排序。

![img](https://static001.geekbang.org/resource/image/55/7e/5535a3d83534byy54ab201e865ec4a7e.jpeg)

设计召回层时，计算速度和召回率其实是两个矛盾的指标。怎么理解呢？比如说，为了提高计算速度，我们需要使召回策略尽量简单，而为了提高召回率或者说召回精度，让召回策略尽量把用户感兴趣的物品囊括在内，这又要求召回策略不能过于简单，否则召回物品就无法满足排序模型的要求。



## 单策略召回

单策略召回指的是，通过制定一条规则或者利用一个简单模型来快速地召回可能的相关物品。 **这里的规则其实就是用户可能感兴趣的物品的特点。**

在推荐电影的时候，我们首先要想到用户可能会喜欢什么电影。按照经验来说，很有可能是这三类，分别是大众口碑好的、近期非常火热的，以及跟我之前喜欢的电影风格类似的。

比如在 SparrowRecSys 中，我就制定了这样一条召回策略：如果用户对电影 A 的评分较高，比如超过 4 分，那么我们就将与 A 风格相同，并且平均评分在前 50 的电影召回，放入排序候选集中。

局限性：因为大多数时候用户的兴趣是非常多元的，他们不仅喜欢自己感兴趣的，也喜欢热门的，当然很多时候也喜欢新上映的。

## 多路召回

所谓“多路召回策略”，就是指采用不同的策略、特征或简单模型，分别召回一部分候选集，然后把候选集混合在一起供后续排序模型使用的策略。多路召回策略是在计算速度和召回率之间进行权衡的结果。

在实现的过程中，为了进一步优化召回效率，我们还可以通过多线程并行、建立标签 / 特征索引、建立常用召回集缓存等方法来进一步完善它。

缺点：比如，**在确定每一路的召回物品数量时，往往需要大量的人工参与和调整，具体的数值需要经过大量线上 AB 测试来决定。**此外，<u>因为策略之间的信息和数据是割裂的，</u>所以我们很难综合考虑不同策略对一个物品的影响。



## 基于 Embedding 的召回方法

事实上，利用物品和用户 Embedding 相似性来构建召回层，是深度学习推荐系统中非常经典的技术方案。

1. 一方面，多路召回中使用的“兴趣标签”“热门度”“流行趋势”“物品属性”等信息都可以作为 Embedding 方法中的附加信息（Side Information），融合进最终的 Embedding 向量中 。因此，在利用 Embedding 召回的过程中，我们就相当于考虑到了多路召回的多种策略。
2. 另一方面，Embedding 召回的评分具有连续性。我们知道，多路召回中不同召回策略产生的相似度、热度等分值不具备可比性，所以我们无法据此来决定每个召回策略放回候选集的大小。但是，Embedding 召回却可以把 Embedding 间的相似度作为唯一的判断标准，因此它可以随意限定召回的候选集大小。
3. 最后，在线上服务的过程中，Embedding 相似性的计算也相对简单和直接。通过简单的点积或余弦相似度的运算就能够得到相似度得分，便于线上的快速召回。

```scala

public static List<Movie> retrievalCandidatesByEmbedding(User user){
    if (null == user){
        return null;
    }
    //获取用户embedding向量
    double[] userEmbedding = DataManager.getInstance().getUserEmbedding(user.getUserId(), "item2vec");
    if (null == userEmbedding){
        return null;
    }
    //获取所有影片候选集(这里取评分排名前10000的影片作为全部候选集)
    List<Movie> allCandidates = DataManager.getInstance().getMovies(10000, "rating");
    HashMap<Movie,Double> movieScoreMap = new HashMap<>();
    //逐一获取电影embedding，并计算与用户embedding的相似度
    for (Movie candidate : allCandidates){
        double[] itemEmbedding = DataManager.getInstance().getItemEmbedding(candidate.getMovieId(), "item2vec");
        double similarity = calculateEmbeddingSimilarity(userEmbedding, itemEmbedding);
        movieScoreMap.put(candidate, similarity);
    }
   
    List<Map.Entry<Movie,Double>> movieScoreList = new ArrayList<>(movieScoreMap.entrySet());
    //按照用户-电影embedding相似度进行候选电影集排序
    movieScoreList.sort(Map.Entry.comparingByValue());


    //生成并返回最终的候选集
    List<Movie> candidates = new ArrayList<>();
    for (Map.Entry<Movie,Double> movieScoreEntry : movieScoreList){
        candidates.add(movieScoreEntry.getKey());
    }
    return candidates.subList(0, Math.min(candidates.size(), size));
}
```

通过三步生成了最终的候选集。

第一步，我们获取用户的 Embedding。

第二步，我们获取所有物品的候选集，并且逐一获取物品的 Embedding，计算物品 Embedding 和用户 Embedding 的相似度。

第三步，我们根据相似度排序，返回规定大小的候选集。

在这三步之中，最主要的时间开销在第二步，虽然它的时间复杂度是线性的，但当物品集过大时（比如达到了百万以上的规模），线性的运算也可能造成很大的时间开销。那有没有什么方法能进一步缩小 Embedding 召回层的运算时间呢？

![img](https://static001.geekbang.org/resource/image/2f/80/2fc1eyyefd964f7b65715de6f896c480.jpeg)

问题：

实现一个多线程版本的多路召回策略吗？

你觉得对于 Embedding 召回来说，我们怎么做才能提升计算 Embedding 相似度的速度？



- 如果新item特别多(每秒可能几十到上百）对item展示的实时性要求较高（小于五分钟就需要曝光）这些item的embedding该如何更新呢？

作者回复: 这还是冷启动问题，我之前有过类似的回复，可以参考。

另外其实我不建议如此多的新item还采用embedding的方案，没有必要，可以考虑在重排层加入这些新鲜的item。用一些基本的特征参与重排，简单模型虽然传统，但在特殊的场景下也经常采用。

- 关于EGES的训练,试了下,由于电商领域商品维度非常大,即使hash后也很大,这导致训练非常慢,这个一般怎么解决啊?

  作者回复: 非常好的业界实践的问题。其实方法无非是我们提到过的几种。

  1、把商品embedding进行预训练，再跟其他side information特征一起输入EGES。
  2、像你说的hash方法 ？？？？什么是hash芳芳
  3、商品的聚类后输入，比如非常类似的商品，可以用一个商品id替代，当作一个商品来处理。这个方法airbnb embedding的论文讲的非常好。

- 如果基于兴趣标签做召回，同一个物品，有多个标签，而用户也计算了出了多个兴趣标签，那么怎么做用户的多兴趣标签与物品的最优匹配呢？还有物品的标签有多层，那么怎么利用上一层的标签呢？

  作者回复: 这是个好问题。简单的做法是把兴趣标签转换成multihot向量，然后就可以计算出用户和物品的相似度了。

  复杂一点也可以计算每个兴趣标签的tfidf，为标签分配权重后，再转换成multihot向量。

  如果标签有多层，也不妨碍把多层标签全部放到multihot向量中，高层标签的权重可以适当降低。这也是思路之一。

- 多路召回中，topk除了根据经验值确定，业界通用的是怎么确定k得大小呢

  作者回复: 在系统延迟允许的情况下，其实k取的越大越好。

  一般来说，如果最后的推荐结果需要n条，k取5-10n是比较合适的。





# 12 | 局部敏感哈希：如何在常数时间内搜索Embedding最近邻？

局部敏感哈希 Locality Sensitive Hashing,LSH

如何快速找到与一个 Embedding 最相似的 Embedding？这直接决定了召回层的执行速度，进而会影响推荐服务器的响应延迟。

假设，用户和物品的 Embeding 都在一个 k 维的 Embedding 空间中，物品总数为 n，那么遍历计算一个用户和所有物品向量相似度的时间复杂度是多少呢？不难算出是 O(k×n)。虽然这一复杂度是线性的，但物品总数 n 达到百万甚至千万量级时，线性的时间复杂度也是线上服务不能承受的。

由于用户和物品的 Embedding 同处一个向量空间内，因此**召回与用户向量最相似的物品 Embedding 向量这一问题，其实就是在向量空间内搜索最近邻的过程。**



## 使用“聚类”还是“索引”来搜索最近邻？

- 一种是聚类，我们把相似的点聚类到一起，就可以快速地找到彼此间的最近邻；

- 一种是索引，比如，我们通过某种数据结构<u>建立基于向量距离的索引</u>，在查找最近邻的时候，通过索引快速缩小范围来降低复杂度。

kmeans聚类的缺点：聚类边缘的点的最近邻往往会包括相邻聚类的点，如果我们只在类别内搜索，就会遗漏这些近似点。此外，中心点的数量 k 也不那么好确定，k 选得太大，离线迭代的过程就会非常慢，k 选得太小，在线搜索的范围还是很大，并没有减少太多搜索时间。所以基于聚类的搜索还是有一定局限性的，解决上面的问题也会增加过多冗余过程，得不偿失。

经典的**向量空间索引方法 Kd-tree**（K-dimension tree）。与聚类不同，**它是为空间中的点 / 向量建立一个索引。**

这该怎么理解呢？

图 3 中的点云，我们先用红色的线把点云一分为二，再用深蓝色的线把各自片区的点云一分为二，以此类推，直到每个片区只剩下一个点，这就完成了空间索引的构建。如果我们能够把这套索引“搬”到线上，就可以利用二叉树的结构快速找到邻接点。比如，希望找到点 q 的 m 个邻接点，我们就可以先搜索它相邻子树下的点，如果数量不够，我们可以向上回退一个层级，搜索它父片区下的其他点，直到数量凑够 m 个为止。

![img](https://static001.geekbang.org/resource/image/df/3f/dfb2c271d9eaa3a29054d2aea24b5e3f.jpeg)

​																		图3 Kd-tree索引

听上去 Kd-tree 索引似乎是一个完美的方案，但它还是无法完全解决边缘点最近邻的问题。对于点 q 来说，它的邻接片区是右上角的片区，但是它的最近邻点却是深蓝色切分线下方的那个点。所以按照 Kd-tree 的索引方法，我们还是会遗漏掉最近邻点，**它只能保证快速搜索到近似的最近邻点集合。**而且 Kd-tree 索引的结构并不简单，离线和在线维护的过程也相对复杂，这些都是它的弊端。那有没有更“完美”的解决方法呢？





## 局部敏感哈希(LSH)的基本原理及多桶策略

### 1.  局部敏感哈希的基本原理

LSH的基本思想是希望**让相邻的点落入同一个“桶”**，这样在进行最近邻搜索时，我们仅需要在一个桶内，或相邻几个桶内的元素中进行搜索即可。如果保持每个桶中的元素个数在一个常数附近，我们就可以把最近邻搜索的时间复杂度降低到常数级别。

如何构建局部敏感哈希中的“桶”呢？以基于欧式距离的最近邻搜索为例，来解释构建局部敏感哈希“桶”的过程。



关键：

如果将高维空间中的点向低维空间进行映射，其欧式相对距离是不是会保持不变呢？以图 4 为例，图 4 中间的彩色点处在二维空间中，当我们把二维空间中的点通过不同角度映射到 a、b、c 这三个一维空间时，可以看到原本相近的点，在一维空间中都保持着相近的距离。而原本远离的绿色点和红色点在一维空间 a 中处于接近的位置，却在空间 b 中处于远离的位置。

![img](https://static001.geekbang.org/resource/image/d9/55/d9476e92e9a6331274e18abc416db955.jpeg)

​																	图4 高维空间点向低维空间映射



> **欧式空间中，将高维空间的点映射到低维空间，原本接近的点在低维空间中肯定依然接近，但原本远离的点则有一定概率变成接近的点。**



利用低维空间可以保留高维空间相近距离关系的性质，我们就可以构造局部敏感哈希“桶”。

对于 Embedding 向量来说，由于 Embedding 大量使用内积操作计算相似度，因此我们也可以用内积操作来构建局部敏感哈希桶。

假设 v 是高维空间中的 k 维 Embedding 向量，x 是随机生成的 k 维映射向量。那我们利用内积操作可以将 v 映射到一维空间，得到数值 h(v)=v⋅x。

一维空间也会部分保存高维空间的近似距离信息。因此，我们可以使用哈希函数 h(v) 进行分桶，公式为：![image-20210224215741391](/Users/michelle/Library/Application Support/typora-user-images/image-20210224215741391.png)。其中， ⌊⌋ 是向下取整操作， **w 是分桶宽度，b 是 0 到 w 间的一个均匀分布随机变量，避免分桶边界固化。**

（目的从高维向低维映射）

不过，<u>映射操作会损失部分距离信息，如果我们仅采用一个哈希函数进行分桶，必然存在相近点误判的情况，因此，我们可以采用 m 个哈希函数同时进行分桶。如果两个点同时掉进了 m 个桶，那它们是相似点的概率将大大增加。</u>通过分桶找到相邻点的候选集合后，我们就可以在有限的候选集合中通过遍历找到目标点真正的 K 近邻了。

本质思想：局部敏感哈希通过分桶方式保留部分距离信息，大规模降低近邻点候选集。

### 2. 局部敏感哈希的多桶策略

使用多个分桶函数的方式来增加找到相似点的概率。但是，如果有多个分桶函数的话，具体应该如何处理不同桶之间的关系呢？这就涉及局部敏感哈希的多桶策略。

> 假设有 A、B、C、D、E 五个点，有 h1和 h2两个分桶函数。使用 h1来分桶时，A 和 B 掉到了一个桶里，C、D、E 掉到了一个桶里；使用 h2来分桶时，A、C、D 掉到了一个桶里，B、E 在一个桶。
>
> 那么请问如果我们想找点 C 的最近邻点，应该怎么利用两个分桶结果来计算呢？如果我们用“且”（And）操作来处理两个分桶结果之间的关系，那么结果是这样的，找到与点 C 在 h1函数下同一个桶的点，且在 h2函数下同一个桶的点，作为最近邻候选点。我们可以看到，满足条件的点只有一个，那就是点 D。也就是说，点 D 最有可能是点 C 的最近邻点。**用“且”操作作为多桶策略，可以最大程度地减少候选点数量。**
>
> 但是，由于哈希分桶函数不是一个绝对精确的操作，点 D 也只是最有可能的最近邻点，不是一定的最近邻点，因此，**“且”操作其实也增大了漏掉最近邻点的概率。**
>
> 那如果我们采用“或”（Or）操作作为多桶策略，又会是什么情况呢？具体操作就是，我们找到与点 C 在 h1函数下同一个桶的点，或在 h2函数下同一个桶的点。这个时候，我们可以看到候选集中会有三个点，分别是 A、D、E。这样一来，**虽然我们增大了候选集的规模，减少了漏掉最近邻点的可能性，但增大了后续计算的开销。**
>
> 当然，局部敏感哈希的多桶策略还可以更加复杂，比如使用 3 个分桶函数分桶，把同时落入两个桶的点作为最近邻候选点等等。



**应该选择“且”操作还是“或”操作，以及到底该选择使用几个分桶函数，每个分桶函数分几个桶呢？这些都还是工程上的权衡问题。？？**



1. 点数越多，我们越应该增加每个分桶函数中桶的个数；相反，点数越少，我们越应该减少桶的个数；
2. <u>Embedding 向量的维度越大，我们越应该增加哈希函数的数量，尽量采用且的方式作为多桶策略；相反，</u>Embedding 向量维度越小，我们越应该减少哈希函数的数量，多采用或的方式作为分桶策略。



局部敏感哈希能在常数时间得到最近邻的结果吗？答案是可以的，如果我们能够精确地控制**每个桶内的点的规模是 C，假设每个 Embedding 的维度是 N，**那么找到最近邻点的时间开销将永远在 O(C⋅N) 量级。采用多桶策略之后，**假设分桶函数数量是 K**，那么时间开销也在 **O(K⋅C⋅N)** 量级，这仍然是一个常数。



## 局部敏感哈希实践

利用 Sparrow Recsys 训练好的物品 Embedding，来实现局部敏感哈希的快速搜索吧。为了保证跟 Embedding 部分的平台统一，这一次我们继续使用 Spark MLlib 完成 LSH 的实现。

在将电影 Embedding 数据转换成 dense Vector 的形式之后，我们使用 Spark MLlib 自带的 LSH 分桶模型 BucketedRandomProjectionLSH（我们简称 LSH 模型）来进行 LSH 分桶。其中最关键的部分是设定 LSH 模型中的 BucketLength 和 NumHashTables 这两个参数。**其中，BucketLength 指的就是分桶公式中的分桶宽度 w，NumHashTables 指的是多桶策略中的分桶次数。**![image-20210224220717133](/Users/michelle/Library/Application Support/typora-user-images/image-20210224220717133.png)

清楚了模型中的关键参数，执行的过程就跟我们讲过的其他 Spark MLlib 模型一样了，**都是先调用 fit 函数训练模型，再调用 transform 函数完成分桶的过程，**具体的实现：

```scala

def embeddingLSH(spark:SparkSession, movieEmbMap:Map[String, Array[Float]]): Unit ={
  //将电影embedding数据转换成dense Vector的形式，便于之后处理
  val movieEmbSeq = movieEmbMap.toSeq.map(item => (item._1, Vectors.dense(item._2.map(f => f.toDouble))))
  val movieEmbDF = spark.createDataFrame(movieEmbSeq).toDF("movieId", "emb")


  //利用Spark MLlib创建LSH分桶模型
  val bucketProjectionLSH = new BucketedRandomProjectionLSH()
    .setBucketLength(0.1)
    .setNumHashTables(3)
    .setInputCol("emb")
    .setOutputCol("bucketId")
  //训练LSH分桶模型
  val bucketModel = bucketProjectionLSH.fit(movieEmbDF)
  //进行分桶
  val embBucketResult = bucketModel.transform(movieEmbDF)
  
  //打印分桶结果
  println("movieId, emb, bucketId schema:")
  embBucketResult.printSchema()
  println("movieId, emb, bucketId data result:")
  embBucketResult.show(10, truncate = false)
  
  //尝试对一个示例Embedding查找最近邻
  println("Approximately searching for 5 nearest neighbors of the sample embedding:")
  val sampleEmb = Vectors.dense(0.795,0.583,1.120,0.850,0.174,-0.839,-0.0633,0.249,0.673,-0.237)
  bucketModel.approxNearestNeighbors(movieEmbDF, sampleEmb, 5).show(truncate = false)
}
```

使用 LSH 模型对电影 Embedding 进行分桶得到的五个结果打印了出来，如下所示：

numhashtable为3，是指使用了3个分桶函数

```scala

+-------+-----------------------------+------------------+
|movieId|emb                          |bucketId          |
+-------+-----------------------------+------------------------+
|710    |[0.04211471602320671,..]     |[[-2.0], [14.0], [8.0]] |
|205    |[0.6645985841751099,...]     |[[-4.0], [3.0], [5.0]]  |
|45     |[0.4899883568286896,...]     |[[-6.0], [-1.0], [2.0]] |
|515    |[0.6064003705978394,...]     |[[-3.0], [-1.0], [2.0]] |
|574    |[0.5780771970748901,...]     |[[-5.0], [2.0], [0.0]]  |
+-------+-----------------------------+------------------------+

```

小结

多桶策略，首先是基于“且”操作的多桶策略能够进一步减少候选集规模，增加计算效率，其次是基于“或”操作的多桶策略则能够提高召回率，减少漏掉最近邻点的可能性。

![img](https://static001.geekbang.org/resource/image/40/b1/40yy632948cdd9090fe34d3957307eb1.jpeg)

思考： 

- 如果让你在推荐服务器内部的召回层实现最近邻搜索过程，你会怎样存储和使用我们在离线产生的分桶数据，以及怎样设计线上的搜索过程呢？

- 倒排索引的思路：以item_id作为key， item_id对应的BucketId作为value存储在redis， 再以每个BucketId作为key， item_id作为value存储在redis， 在召回的时候遍历item_id的所有BucketId，获取BucketId对应的item_id就是需要召回的item



- b 是 0 到 w 间的一个均匀分布随机变量，避免分桶边界固化。这是什么意思呢？是说可以通过调整b来形成另外一个一个hash函数？

- 因为如果总是固定边界，很容易让边界两边非常接近的点总是被分到两个桶里。这是我们不想看到的。

所以随机调整b，生成多个hash函数，并且采用或的方式组合，就可以一定程度避免这些边界点的问题。





















































































































































