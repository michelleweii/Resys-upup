https://time.geekbang.org/column/article/295939

### 04 ###

特征是对某个行为过程相关信息的抽象表达：因为一个行为过程必须转换成某种数学形式才能被机器学习模型所学习。

但也不能所有情节都当做特征放进模型学习。具体的推荐场景中包含大量冗余的、无用的信息，把它们都考虑进来甚至会损害模型的泛化能力。

> 构建推荐系统特征工程的原则：尽可能地让特征工程抽取出的一组特征，能够保留推荐环境及用户行为过程中的所有“有用“信息，并且尽量摒弃冗余信息。



电影推荐的要素和特征化方式

![img](https://static001.geekbang.org/resource/image/af/5d/af921c7e81984281621729f6e75c1b5d.jpeg)



#### 推荐系统中的常用特征 ####

1. 用户行为数据（最常用、最关键）
2. 用户关系数据
3. 属性、标签类数据
4. 内容类数据
5. 场景信息（上下文信息）



1、用户行为数据

用户行为在推荐系统中一般分为显性反馈行为（Explicit Feedback）和隐性反馈行为（Implicit Feedback）两种。

不同业务场景下用户行为数据的例子

![img](https://static001.geekbang.org/resource/image/75/06/7523075958d83e9bd08966b77ea23706.jpeg)

隐性反馈行为更重要，主要原因是显性反馈行为的收集难度过大，数据量小。



2、用户关系数据

物以类聚，人以群分

用户行为数据是人与物之间的“连接”日志；

用户关系数据就是人与人之间连接的记录

用户关系数据也可以分为“显性”和“隐性”两种，或者称为“强关系”和“弱关系”。用户与用户之间可以通过“关注”“好友关系”等连接建立“强关系”，也可以通过“互相点赞”“同处一个社区”，甚至“同看一部电影”建立“弱关系”。

- 比如可以将用户关系作为召回层的一种物品召回方式；（user-cf）
- 也可以通过用户关系建立关系图，使用 Graph Embedding 的方法生成用户和物品的 Embedding；
- 还可以直接利用关系数据，通过“好友”的特征为用户添加新的属性特征；
- 甚至可以利用用户关系数据直接建立社会化推荐系统。



3、属性、标签类数据

推荐系统中另外一大类特征来源是属性、标签类数据，本质上都是直接描述用户或者物品的特征。属性和标签的主体可以是用户，也可以是物品

图：属性、标签类数据的分类和来源

![img](https://static001.geekbang.org/resource/image/ba/69/ba044e0033b513d996633de77e11f969.jpeg)



在推荐系统中使用属性、标签类数据，一般是通过 Multi-hot 编码的方式将其转换成特征向量，一些重要的属性标签类特征也可以先转换成 Embedding，比如业界最新的做法是将标签属性类数据与其描述主体一起构建成知识图谱（Knowledge Graph），在其上施以 Graph Embedding 或者 GNN（Graph Neural Network，图神经网络）生成各节点的 Embedding，再输入推荐模型。



4、内容类数据

相比标签类特征，内容类数据往往是大段的描述型文字、图片，甚至视频。文字信息则更多是通过自然语言处理的方法提取关键词、主题、分类等信息，一旦这些特征被提取出来，就跟处理属性、标签类特征的方法一样，通过 Multi-hot 编码，Embedding 等方式输入推荐系统进行训练。



5、场景信息（上下文信息）

最后一大类是场景信息，或称为上下文信息（Context），它是描述推荐行为产生的场景的信息。最常用的上下文信息是“时间”和通过 GPS、IP 地址获得的“地点”信息。根据推荐场景的不同，上下文信息的范围极广，除了我们上面提到的时间和地点，还包括“当前所处推荐页面”“季节”“月份”“是否节假日”“天气”“空气质量”“社会大事件”等等。





### 05 ###

> 知道了推荐系统要使用的常用特征有哪些。但这些原始的特征是无法直接提供给推荐模型使用的，因为推荐模型本质上是一个函数，输入输出都是数字或数值型的向量。那么问题来了，像动作、喜剧、爱情、科幻这些电影风格，是怎么转换成数值供推荐模型使用的呢？用户的行为历史又是怎么转换成数值特征的呢？



#### spark ####

Spark 是一个分布式计算平台。所谓分布式，指的是计算节点之间不共享内存，需要通过网络通信的方式交换数据。Spark 最典型的应用方式就是建立在大量廉价的计算节点上，这些节点可以是廉价主机，也可以是虚拟的 Docker Container（Docker 容器）。

Spark 程序由 Manager Node（管理节点）进行调度组织，由 Worker Node（工作节点）进行具体的计算任务执行，最终将结果返回给 Drive Program（驱动程序）。在物理的 Worker Node 上，数据还会分为不同的 partition（数据分片），可以说 partition 是 Spark 的基础数据单元。

![img](https://static001.geekbang.org/resource/image/4a/9b/4ae1153e4daee39985c357ed796eca9b.jpeg)





如何利用 One-hot 编码处理类别型特征

广义上来讲，所有的特征都可以分为两大类。

第一类是类别、ID 型特征——影的风格、ID、标签、导演演员等信息，用户看过的电影 ID、用户的性别、地理位置信息、当前的季节、时间（上午，下午，晚上）、天气等等，这些无法用数字表示的信息全都可以被看作是类别、ID 类特征。

第二类是数值型特征——能用数字直接表示的特征就是数值型特征，典型的包括用户的年龄、收入、电影的播放时长、点击量、点击率等。

> 进行特征处理的目的，是把所有的特征全部转换成一个数值型的特征向量。

对于数值型特征，这个过程非常简单，直接把这个数值放到特征向量上相应的维度上就可以了。但是对于类别、ID 类特征，我们应该怎么处理它们呢？



one-hot——将类别、**ID 型特征**转换成数值向量的一种最典型的编码方式

类别转换：

![img](https://static001.geekbang.org/resource/image/94/15/94f78685d98671648638e330a461ab15.jpeg)

id类转换：

ID 型特征也经常使用 One-hot 编码。比如，用户 U 观看过电影 M，这个行为是一个非常重要的用户特征，那我们应该如何向量化这个行为呢？其实也是使用 One-hot 编码。**假设，我们的电影库中一共有 1000 部电影，电影 M 的 ID 是 310（编号从 0 开始），那这个行为就可以用一个 1000 维的向量来表示，让第 310 维的元素为 1，其他元素都为 0。**

One-hot 编码也可以自然衍生成 **Multi-hot 编码（多热编码）**。比如，对于历史行为序列类、标签特征等数据来说，用户往往会与多个物品产生交互行为，或者一个物品被打上多个标签，这时最常用的特征向量生成方式就是把其转换成 Multi-hot 编码。因为每个电影都是有多个 Genre（风格）类别的，所以我们就可以用 Multi-hot 编码完成标签到向量的转换。

> multi-hot e.g. :
>
> 用户行为特征是multi-hot的，即多值离散特征。针对这种特征，由于每个涉及到的非0值个数是不一样的，常见的做法就是将id转换成embedding之后，加一层pooling层，比如average-pooling，sum-pooling，max-pooling。DIN中使用的是weighted-sum，其实就是加权的sum-pooling，权重经过一个activation unit计算得到。



数值型特征的处理 - *归一化和分桶*

一是特征的尺度，二是特征的分布



> 用分桶的方式来解决特征值分布极不均匀的问题。所谓“分桶（Bucketing）”，就是将样本按照某特征的值从高到低排序，然后按照桶的数量找到分位数，将样本分到各自的桶中，再用桶 ID 作为特征值。

怎么理解？比如用户对电影的打分都是3-4之间（5分制）比如[3,3.3)、 [3.3,3.6) 和 [3.6,5]三个区间对应分成三类。这样用户所打的分数就更有区分性。

> **1）分桶。**比如视频一周内被播放次数应该是一个有用的特征，因为播放次数跟视频的热度有很强的相关性，**但是如果不同视频的播放次数跨越不同的数量级，则很难发挥想要的作用。**例如 LR 模型，模型往往只对比较大的特征值敏感。对于这种情况，通常的解决方法是进行分桶。***分桶操作可以看作是对数值变量的离散化，之后通过二值化进行 one-hot 编码。***
>
> 分桶的数量和宽度可以根据业务领域的经验来指定，也有一些常规做法。(1)等距分桶，每个桶的值域是固定的，这种方式适用于样本分布较为均匀的情况；(2)等频分桶，即使得每个桶里数据一样多，这种方式可以保证每个桶有相同的样本数，但也会出现特征值差异非常大的样本被放在一个桶中的情况；(3)模型分桶，使用模型找到最佳分桶，例如利用聚类的方式将特征分成多个类别，或者树模型，这种非线性模型天生具有对连续型特征切分的能力，利用特征分割点进行离散化。
>
> **分桶是离散化的常用方法，将连续特征离散化为一系列 0/1 的离散特征，离散化之后得到的稀疏向量，内积乘法运算速度更快，计算结果方便存储。**离散化之后的特征对于异常数据也具有很强的鲁棒性。需要注意的是：1)要使得桶内的属性取值变化对样本标签的影响基本在一个不大的范围，即不能出现单个分桶的内部，样本标签输出变化很大的情况；2)使每个桶内都有足够的样本，如果桶内样本太少，则随机性太大，不具有统计意义上的说服力；3)每个桶内的样本尽量分布均匀。
>
> ​    常用的行为次数与曝光次数比值类的特征，由于数据的稀疏性，这种计算方式得到的统计量通常具有较大的偏差，需要做**平滑处理**，比如广告点击率常用的贝叶斯平滑技术。而 <u>在我们推荐场景中，也会用到很多统计类特征、比率特征。如果直接使用，比如由于不同item的下发量是不同的，这会让推荐偏向热门的类目，使得越推越窄，无法发现用户的个体差异，也不利于多样性的探索。</u> 常见的有贝叶斯平滑和威尔逊区间平滑等。



- 离散特征：比如西瓜的色泽有 [青绿、乌黑、浅白] 三种颜色；或许有时候也称为属性类;
- 标签类特征：也是离散的类别特征；
- 连续特征：比如西瓜的含糖率有 0.460、0.376、0.263、0.821、0.102 ... 等等等；
- 数值型特征：特征以数字形式表示？
- ID类特征：也是离散特征，电商领域为例，存在大量ID类特征，比如user ID, item ID, product ID, store ID, brand ID和category ID等。



spark分桶的特征处理 QuantileDiscretizer

![img](https://static001.geekbang.org/resource/image/b3/7b/b3b8c959df72ce676ae04bd8dd987e7b.jpeg)



### 06 word2vec ###

Embedding 就是用一个数值向量“表示”一个对象（Object）的方法。对序列数据进行了 Embedding 化。

大量使用 One-hot 编码会导致样本特征向量极度稀疏，而深度学习的结构特点又不利于稀疏特征向量的处理，<u>*因此几乎所有深度学习推荐模型都会由 Embedding 层负责将稀疏高维特征向量转换成稠密低维特征向量。*</u>

![img](https://static001.geekbang.org/resource/image/99/39/9997c61588223af2e8c0b9b2b8e77139.jpeg)

它的输入层和输出层的维度都是 V，这个 V 其实就是语料库词典的大小。假设语料库一共使用了 10000 个词，那么 V 就等于 10000。根据生成的训练样本，这里的输入向量自然就是由输入词转换而来的 One-hot 编码向量，**输出向量则是由多个输出词转换而来的 Multi-hot 编码向量**，显然，基于 Skip-gram 框架的 Word2vec 模型解决的是一个多分类问题。

最后是激活函数的问题，这里我们需要注意的是，隐层神经元是没有激活函数的，或者说采用了输入即输出的恒等函数作为激活函数，而输出层神经元采用了 **<u>*softmax 作为激活函数*</u>**。

为什么要这样设置 Word2vec 的神经网络，以及我们为什么要这样选择激活函数呢？因为这个神经网络其实是为了表达从输入向量到输出向量的这样的一个条件概率关系，我们看下面的式子：

![image-20210118233633279](/Users/michelle/Library/Application Support/typora-user-images/image-20210118233633279.png)

这个由输入词 WI 预测输出词 WO 的条件概率，其实就是 Word2vec 神经网络要表达的东西。我们通过极大似然的方法去最大化这个条件概率，就能够让相似的词的内积距离更接近，这就是我们希望 Word2vec 神经网络学到的。

> 多分类问题：



怎样把词向量从 Word2vec 模型中提取出来？

Embedding 藏在输入层到隐层的权重矩阵 W VxN 中。在训练完成后，模型输入向量矩阵的行向量，就是我们要提取的词向量。

![img](https://static001.geekbang.org/resource/image/0d/72/0de188f4b564de8076cf13ba6ff87872.jpeg)

在实际的使用过程中，我们往往会**把输入向量矩阵转换成词向量查找表（Lookup table）**。例如，输入向量是 10000 个词组成的 One-hot 向量，隐层维度是 300 维，那么输入层到隐层的权重矩阵为 10000x300 维。在转换为词向量 Lookup table 后，每行的权重即成了对应词的 Embedding 向量。如果我们把这个查找表存储到线上的数据库中，就可以轻松地在推荐物品的过程中使用 Embedding 去计算相似性等重要的特征了。

![img](https://static001.geekbang.org/resource/image/1e/96/1e6b464b25210c76a665fd4c34800c96.jpeg)

​                                                                      Word2vec的Lookup table



#### item2vec

既然 Word2vec 可以对词“序列”中的词进行 Embedding，那么对于用户购买“序列”中的一个商品，用户观看“序列”中的一个电影，也应该存在相应的 Embedding 方法。

![img](https://static001.geekbang.org/resource/image/d8/07/d8e3cd26a9ded7e79776dd31cc8f4807.jpeg)

图8 不同场景下的序列数据



### 07 利用图结构数据生成Graph Embedding？

![img](https://static001.geekbang.org/resource/image/54/91/5423f8d0f5c1b2ba583f5a2b2d0aed91.jpeg)

（1）从社交网络中，我们可以发现意见领袖，可以发现社区，再根据这些“社交”特性进行社交化的推荐，如果我们可以对社交网络中的节点进行 Embedding 编码，社交化推荐的过程将会非常方便。

（2）知识图谱中包含了不同类型的知识主体（如人物、地点等），附着在知识主体上的属性（如人物描述，物品特点），以及主体和主体之间、主体和属性之间的关系。如果我们能够对知识图谱中的主体进行 Embedding 化，就可以发现主体之间的潜在关系，这对于基于内容和知识的推荐系统是非常有帮助的。

（3）行为关系类图数据。这类数据几乎存在于所有互联网应用中，它事实上是由用户和物品组成的“二部图”（也称二分图，如图 1c）。用户和物品之间的相互行为生成了行为关系图。借助这样的关系图，我们自然能够利用 Embedding 技术发掘出物品和物品之间、用户和用户之间，以及用户和物品之间的关系，从而应用于推荐系统的进一步推荐。



#### 基于随机游走的 Graph Embedding 方法：Deep Walk

> 它的主要思想是在由物品组成的图结构上进行随机游走，产生大量物品序列，然后将这些物品序列作为训练样本输入 Word2vec 进行训练，最终得到物品的 Embedding。因此，DeepWalk 可以被看作连接序列 Embedding 和 Graph Embedding 的一种过渡方法。

![img](https://static001.geekbang.org/resource/image/1f/ed/1f28172c62e1b5991644cf62453fd0ed.jpeg)



Embedding 是如何应用在推荐系统的特征工程中的？

（1）“直接应用”最简单，就是在我们得到 Embedding 向量之后，直接利用 Embedding 向量的相似性实现某些推荐系统的功能。典型的功能有，利用物品 Embedding 间的相似性实现相似物品推荐，利用物品 Embedding 和用户 Embedding 的相似性实现“猜你喜欢”等经典推荐功能，还可以利用物品 Embedding 实现推荐系统中的召回层等。

（2）“预训练应用”指的是在我们预先训练好物品和用户的 Embedding 之后，不直接应用，而是把这些 Embedding 向量作为特征向量的一部分，跟其余的特征向量拼接起来，作为推荐模型的输入参与训练。这样做能够更好地把其他特征引入进来，让推荐模型作出更为全面且准确的预测。

（3）“End2End 应用”，也就是端到端训练，就是指我们不预先训练 Embedding，而是把 Embedding 的训练与深度学习推荐模型结合起来，采用统一的、端到端的方式一起训练，直接得到包含 Embedding 层的推荐模型。这种方式非常流行，比如图 6 就展示了三个包含 Embedding 层的经典模型，分别是微软的 Deep Crossing，UCL 提出的 FNN 和 Google 的 Wide&Deep。

![img](https://static001.geekbang.org/resource/image/e9/78/e9538b0b5fcea14a0f4bbe2001919978.jpg)

图6 带有Embedding层的深度学习模型



#### 基于随机游走的 Graph Embedding 方法：Deep Walk



#### 在同质性和结构性间权衡的方法，Node2vec



总结

- Deep Walk ：首先，我们基于原始的用户行为序列来构建物品关系图，然后采用随机游走的方式随机选择起始点，重新产生物品序列，最后将这些随机游走生成的物品序列输入 Word2vec 模型，生成最终的物品 Embedding 向量。

- Node2vec 相比于 Deep Walk，增加了随机游走过程中跳转概率的倾向性。如果倾向于宽度优先搜索，则 Embedding 结果更加体现“结构性”。如果倾向于深度优先搜索，则更加体现“同质性”。

![img](https://static001.geekbang.org/resource/image/d0/e6/d03ce492866f9fb85b4fbf5fa39346e6.jpeg)





### 08 如何使用Spark生成Item2vec和Graph Embedding？







### 28 YoutubeDNN

