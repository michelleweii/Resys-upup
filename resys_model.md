### 28 YoutubeDNN

1. Youtube是ugc平台，种类风格繁多，头部效应没那么明显（头部效应）；
2. YouTube 的视频基数巨大，用户难以发现喜欢的内容。

#### 候选集生成模型——用于视频召回的候选集生成模型

![img](https://static001.geekbang.org/resource/image/69/cf/6968873184cf93194aa476398c2e35cf.jpg)

最底层是它的输入层，输入的特征包括用户历史观看视频的 Embedding 向量，以及搜索词的 Embedding 向量。

**对于这些 Embedding 特征，YouTube 是利用用户的观看序列和搜索序列，采用了类似 Item2vec 的预训练方式生成的。当然，我们也完全可以采用 Embedding 跟模型在一起 End2End 训练的方式来训练模型。**至于预训练和 End2End 训练这两种方式孰优孰劣，可以自己再深入思考一下。

除了视频和搜索词 Embedding 向量，特征向量中还包括用户的地理位置 Embedding、年龄、性别等特征。**平方处理样本年龄这个特征，为了挖掘特征非线性的特性，当然，这种对连续型特征的处理方式不仅限于平方，其他诸如开方、Log、指数等操作都可以用于挖掘特征的非线性特性。**

三层 ReLU 神经网络过后，YouTube 又使用了 softmax 函数作为输出层。值得一提的是，**这里的输出层不是要预测用户会不会点击这个视频，而是要预测用户会点击哪个视频，**这就跟我们之前实现过的深度推荐模型不一样了。

> 比如说，YouTube 上有 100 万个视频，因为输出层要预测用户会点击哪个视频，所以这里的 sofmax 就有 100 万个输出。因此，**这个候选集生成模型的最终输出，就是一个在所有候选视频上的概率分布。为什么要这么做呢？它其实是为了更好、更快地进行线上服务。**

#### 候选集生成模型独特的线上服务方法

> **为什么候选集生成模型要用“视频 ID”这个标签，来代替“用户会不会点击视频”这个标签作为预测目标。**事实上，这跟候选集生成模型独特的线上服务方式紧密相关。

![img](https://static001.geekbang.org/resource/image/e9/69/e9a20bc7260296e09078509e3f42df69.jpg)

​                                                                                                图3 模型服务部分示意图

具体来说，在模型服务过程中，网络结构比较复杂，如果我们对每次推荐请求都端到端地运行一遍模型，处理一遍候选集，那模型的参数数量就会巨大，整个推断过程的开销也会非常大。

**因此，在通过“候选集生成模型”得到用户和视频的 Embedding 后，我们再通过 Embedding 最近邻搜索的方法，就可以提高模型服务的效率了。这样一来，我们甚至不用把模型推断的逻辑搬上服务器，只需要将用户 Embedding 和视频 Embedding 存到特征数据库就行了。**再加上可以使用局部敏感哈希这类快速 Embedding 查找方法，这对于百万量级规模的候选集生成过程的效率提升是巨大的。

*这里的用户 Embedding 和视频 Embedding 到底是从哪里来的呢？*

**这个问题的答案就是，候选集生成模型为什么要用视频 ID 作为多分类输出的答案了。**我们再仔细看一下图 2 的架构，架构图中从 softmax 向模型服务模块画了个箭头，**用于代表视频 Embedding 向量的生成。**这个问题的答案就是，候选集生成模型为什么要用视频 ID 作为多分类输出的答案了。

**由于最后的输出层是 softmax，而这个 softmax 层的参数本质上就是一个 m x n 维的矩阵，其中 m 指的是最后一层红色的 ReLU 层的维度 m，n 指的是分类的总数，也就是 YouTube 所有视频的总数 n。因此，视频 Embedding 就是这个 m x n 维矩阵的各列向量。**（这样的 Embedding 生成方法其实和 word2vec 中词向量的生成方法是相同的）

用户 Embedding 的生成就非常好理解了，因为输入的特征向量全部都是用户相关的特征，一个物品和场景特征都没有，所以在使用某用户 u 的特征向量作为模型输入时，**最后一层 ReLU 层的输出向量就可以当作该用户 u 的 Embedding 向量。**

在模型训练完成后，逐个输入所有用户的特征向量到模型中，YouTube 就可以得到<u>所有用户的 Embedding 向量，之后就可以把它们预存到线上的特征数据库中了</u>。

在预测某用户的视频候选集时，**YouTube 要先从特征数据库中拿到该用户的 Embedding 向量**，**再在视频 Embedding 向量空间中，利用局部敏感哈希等方法搜索该用户 Embedding 向量的 K 近邻**，这样就可以快速得到 k 个候选视频集合。这就是整个候选集生成模型的训练原理和服务过程。

<u>*问题？user embedding不是实时生成的吗？*</u>



#### youtubeDNN排序模型





### 29 GraphSAGE

### 30 Flink

### 31 DIN

32 强化学习

略

### 33 技术权衡

