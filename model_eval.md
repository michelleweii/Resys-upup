# 26 | 在线测试：如何在推荐服务器内部实现A/B测试？

线上ab测试主要为了解决线下离线评估无法模拟出真实的指标。

线上 A/B 测试都是验证新模型、新功能、新产品是否能够提升效果的主要测试方法。

它通过把被测对象随机分成 A、B 两组，分别对它们进行对照测试的方法得出实验结论。

> 推荐系统中流程是这样的：先将用户**随机**分成实验组和对照组，然后给实验组的用户施以新模型，给对照组的用户施以旧模型，再经过一定时间的测试后，**计算出实验组和对照组各项线上评估指标**，来比较新旧模型的效果差异，最后挑选出效果更好的推荐模型。

**对照组是旧模型；**

**实验组是新模型；**

优点

1、**离线评估无法完全还原线上的工程环境；**

2、**线上系统的某些商业指标在离线评估中无法计算。**

线评估一般是针对模型本身进行评估的，无法直接获得与模型相关的其他指标，特别是商业指标。离线评估关注的往往是 ROC 曲线、PR 曲线的改进，而线上评估却可以全面了解推荐模型带来的用户点击率、留存时长、PV 访问量这些指标的变化。

3、**离线评估无法完全消除数据有偏（Data Bias）现象的影响。**

什么叫“数据有偏”呢？因为离线数据都是系统利用当前算法生成的数据，因此这些数据本身就不是完全客观中立的，**它是用户在当前模型下的反馈。**所以说，**用户本身有可能已经被当前的模型“带跑偏了”**，你再用这些有偏的数据来衡量你的新模型，得到的结果就可能不客观。



## A/B 测试的“分桶”和“分层”原则

问题：比如到底怎样才能对用户进行一个公平公正的分桶呢？如果有多组实验在同时做 A/B 测试，怎样做才能让它们互不干扰？

在 A/B 测试分桶的过程中，需要注意：样本的独立性和分桶过程的无偏性。这里的“独立性”指的是同一个用户在测试的全程只能被分到同一个桶中。“无偏性”指的是在分桶过程中用户被分到哪个实验桶中应该是**一个纯随机的过程。**

在 A/B 测试分层的过程中，在实际的 A/B 测试场景下，同一个网站或应用往往要同时进行多组不同类型的 A/B 测试。比如，前端组正在进行不同 App 界面的 A/B 测试的时候，后端组也在进行不同中间件效率的 A/B 测试，同时算法组还在进行推荐场景 1 和推荐场景 2 的 A/B 测试。这个时候问题就来了，这么多 A/B 测试同时进行，我们怎么才能让它们互相不干扰呢？

**——层与层之间的流量“正交”，同层之间的流量“互斥”。**



### 层与层之间的流量“正交”

它指的是层与层之间的独立实验的流量是正交的，一批实验用的流量穿越每层实验时，都会再次随机打散，然后再用于下一层的实验。

我们来看下面的示意图。假设，在一个 X 层的实验中，流量被随机平均分为 X1（蓝色）和 X2（白色）两部分。当它们穿越到 Y 层的实验之后，X1 和 X2 的流量会被随机且均匀地分配给 Y 层的两个桶 Y1 和 Y2。

如果 Y1 和 Y2 的 X 层流量分配不均匀，那么 Y 层的样本就是有偏的，Y 层的实验结果就会被 X 层的实验影响，也就无法客观地反映 Y 层实验组和对照组变量的影响。

![img](https://static001.geekbang.org/resource/image/e0/7b/e0da06ee473e3f551ac2cyy987957d7b.jpeg)

​                                                                             层与层之间流量正交示例

### 同层之间的流量“互斥”

如果同层之间进行多组 A/B 测试，不同测试之间的流量不可以重叠，这是第一个“互斥”；

一组 A/B 测试中实验组和对照组的流量是不重叠的，这是第二个“互斥”。

在基于用户的 A/B 测试中，“互斥”的含义可以被进一步解读为，不同实验之间以及 A/B 测试的实验组和对照组之间的用户是不重叠的。特别是对推荐系统来说，用户体验的一致性是非常重要的。也就是说我们不可以让同一个用户在不同的实验组之间来回“跳跃”，这样会严重损害用户的实际体验，也会让不同组的实验结果相互影响。因此在 A/B 测试中，保证同一用户始终分配到同一个组是非常有必要的。



## 线上 A/B 测试的评估指标

A/B 测试是模型上线前的最后一道测试，通过 A/B 测试检验的模型会直接服务于线上用户，来完成公司的商业目标。因此，A/B 测试的指标应该与线上业务的核心指标保持一致。

![image-20210222231629879](/Users/michelle/Library/Application Support/typora-user-images/image-20210222231629879.png)



## A/B 测试的实现方法

```java

public class ABTest {
    final static int trafficSplitNumber = 5;
    final static String bucketAModel = "emb";  // 实验组
    final static String bucketBModel = "nerualcf"; // 对照组
    final static String defaultModel = "emb"; // 其余用户
    public static String getConfigByUserId(String userId){
        if (null == userId || userId.isEmpty()){
            return defaultModel;
        }
        // 利用 userId 的 hashCode 把数值型的 ID 打散
        // 利用 userId 的 hashCode 和 trafficSplitNumber 这个参数进行取余数的操作
        //根据余数的值来确定 userId 在哪一个实验组里。
      
        // trafficSplitNumber将全部用户分成几份；
        // 我们把所有用户分成了 5 份，让第 1 份用户参与 A 组实验，第 2 份用户参与 B 组实验，其余用户继续使用系统的默认设置。这样的操作就是分流操作，也就是把流量划分之后，选取一部分参与 A/B 测试。
        if(userId.hashCode() % trafficSplitNumber == 0){
            System.out.println(userId + " is in bucketA.");
            return bucketAModel;
        }else if(userId.hashCode() % trafficSplitNumber == 1){
            System.out.println(userId + " is in bucketB.");
            return bucketBModel;
        }else{
            System.out.println(userId + " isn't in AB test.");
            return defaultModel;
        }
    }
}
```

总结

![image-20210222230904732](/Users/michelle/Library/Application Support/typora-user-images/image-20210222230904732.png)



如果我们在测试模型的时候，一个实验是在首页测试新的推荐模型，另一个实验是在内容页测试新的推荐模型，你觉得这两个实验应该放在同一层，还是可以放在不同的层呢？为什么？

应该放在同一层，因为首页推荐可能会把一些有兴趣偏好的用户导入到对应的内容页，比如首页推荐球鞋，对于想购买球鞋的就会进入到球鞋内容页，这样对于内容页推荐来说 ，用户不是随机，是有偏的。